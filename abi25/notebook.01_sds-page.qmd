---
title: "Notebook 1 - Bestimmung der Proteinmasse mittels SDS-PAGE"
subtitle: "Praktikum: Angewandte Bioinformatik" 
author: M.Sc. Florian Heinke (heinke@hs-mittweida.de)
format:
  html:
    theme: lumen
    code-tools:
      source: false
      toggle: false
      caption: none
    code-block-border-left: "#0069B3"
    code-block-bg: true
    toc: true
    toc-location: left
    toc-depth: 2
    fig-format: svg
df-print: paged
lang: de
---

# Die Problemstellung

Die [SDS-PAGE (Sodium Dodecyl Sulfate Polyacrylamide Gel Electrophoresis)](https://de.wikipedia.org/wiki/SDS-PAGE) ist eine kosteneffiziente und einfache Methode zur Bestimmung von Proteinmassen (bzw. deren Peptidketten im speziellen). In diesem Praktikum versuchen wir eine Massebestimmung eines uns ungekannten Proteins. Die folgende Abbildung zeigt den Lichtbild-Scan des SDS-PAGE-Gels nach dem Trennlauf. Links: Der Scan ohne Annotationen, rechts: der Scan mit Annotationen. 

![](data/scan.png){fig-align="center"}


Rechts hervorgehoben sind die *Lanes* der Marker-Proteine (mit zwei Replikaten), inklusive deren Massen, sowie die *Lane* des rohen Zellextrakts und jeweils eine *Lane* mit aufgereinigtem Protein (IAC: aufgereinigt mittels Ionenaustausch-Chromatographie, IAC+HIC: aufgereinigt mittels IAC und anschließender hydrophober Interaktionschromatographie.) Anhand der Laufstrecken der Markerproteine, deren Massen, und den Laufstecken des zu charakterisierenden Proteins (dessen Gelbanden sind mit Pfeilen hervorgehoben), werden wir die Massen des Proteins bestimmen. Die Laufstrecken wurden mittels ImageJ (einen Vorläufer von [Fiji](https://fiji.sc/)) bestimmt. Hierfür laden wir zuerst die nötigen Daten aus den heruntergeladenen Datenquellen, bestimmen anschließend eine Kalibriergerade anhand der Markerproteine, und nutzen die das resultierende lineare Regressionsmodell zur Masseschätzung des unbekannten Proteins. 

# Laden der Daten

Wir laden zuerst die Laufweiten und Massen des Markerproteins aus der soeben heruntergeladenen Tabelle:

```{r}
d_marker <- read.table("data/marker_orig.txt", header = T)

```

Hier ist die kleine Tabelle:

```{r}
print(d_marker)
```

Die Spaltenbezeichner stehen für die Molekulare Masse (engl.: *molecular weight*) und Laufstrecke (engl.: *migration distance*)

Nun laden wir die Laufweiten des unbekannten Proteins.

```{r}
d_prot <- read.table("data/sample_orig.txt", header = T)
```

```{r}
print( d_prot )
```

# Berechnung eines linearen Regressionsmodells

Aus Grundlagenveranstaltungen sollte Ihnen bekannt sein, dass die Laufgeschwindigkeit eines Moleküls durch ein Elektrophoresegel nicht-linear mit seiner Masse korreliert. Genauer: Sei $m$ die Masse eines Moleküls, dann ist dessen Geschwindigkeit $v$ negativ exponentiell zur Masse. Wir können den Zusammenhang so schreiben:

$$
v \propto \exp(-m)
$$

Je größer und schwerer ein Protein ist, desto langsamer läuft es durch das Gel (systematische Störgrößen in Gelzusammensetzung und Ladungsverteilung im linearisierten Protein lassen wir außen vor). Aus dem Zusammenhang zwischen Laufstrecke, Zeit und Geschwindigkeit ($s = vt$) und der Tatsache, dass alle Moleküle in einer SDS-PAGE gleichzeitig aufgetrennt werden ("Startzeit" und "Endzeit" sind identisch), folgt:

$$
s \propto \exp(-m)
$$

Diesen Zusammenhang zeigt das folgende Diagramm recht gut.

```{r}
plot(d_marker$MW, d_marker$MD, type = "b", xlab = "Molekulare Masse (kDa)", ylab = "Laufstrecke (pxl)")
```

Entsprechend muss demnach der Logarithmus der Masse linear zur Laufstrecke sein. Berechnen wir den Logarithmus ...


```{r}
d_marker$log.MW <- log( d_marker$MW )
```

und tragen ihn graphisch auf, sehen wir diesen linearen Zusammenhang.


```{r}
plot(d_marker$log.MW, d_marker$MD, type = "b", xlab = "Molekulare Masse (kDa)", ylab = "Laufstrecke (pxl)")

```

Diese Linearität macht man sich zu Nutze wenn man, so wie in unserem Fall, die Masse parallel gelaufener Proteine anhand derer erfassten Laufstrecken bestimmen möchte. Hierfür "invertieren" wir den obigen Zusammenhang und erhalten, dass die logarithmierte Masse eines Proteins, $\log(m)$, durch die folgende lineare Funktion der Laufstrecke $s$ beschrieben werden kann:  

$$
\log(m) = \beta s + \alpha
$$

Die Parameter $\beta$ und $\alpha$ sind die Koeffizienten der linearen Funkion.

Da wir hier experimentelle und somit verrauschte Daten haben, können wir diesen Zusammenhang statistisch nur in Näherung bestimmen: 

$$
\log(m) \approx \beta s + \alpha
$$

Dieses statistische Problem können wir mit Hilfe eines linearen, einfachen Regressionsmodells angehen. D.h. wir bestimmen jene *Funktionsparameter* $\beta$ und $\alpha$ die "möglichst gut" (maximalwahrscheinlich) die gegebenen Daten erklären.

Das geht in `R` ganz einfach mit Hilfe der `lm`-Funktion:

```{r}
regrmodel <- lm(log.MW ~ MD, data = d_marker)
```

Printen wir eine Zusammenfassung unseres Modells, erhalten wir viele interessante Kennzahlen, die wir hier allerdings außer Acht lassen:

```{r}
summary( regrmodel )
```


Für uns sind allerdings nur die Koeffizienten entscheidend:

```{r}
print( regrmodel )
```

Die Zahl `MD` mit einem Wert von $-0{,}003$ entspricht dem Anstieg der linearen Regressionsfunktion, $\beta$. $\alpha$ ist der Achsenabschnittskoeffizient von $5{,}07$.

# Massebestimmung

## naiv mittels der Koeffizienten

Bestimmen wir die Masseschätzungen (Achtung: Es sind zwei Schätzungen, denn wir haben zwei Banden aus unterschiedlichen Aufreinigungsschritten vorliegen!). Das machen wir hier naiv mittels der ermittelten Koeffizienten, bevor wir im nächsten Abschnitt `R`-eigene Funktionen verwenden.

Extrahieren wir die Koeffizienten des Modells. Zuerst den Achsenabschnittskoeffizienten $alpha$.

```{r}
alpha <- regrmodel$coefficients["(Intercept)"]
print(alpha)
```

Und nun den Anstieg $\beta$;

```{r}
beta <- regrmodel$coefficients["MD"]
print(beta)
```

Wir erhalten die Schätzungen, in dem wir die lineare Regressionsfunktion $\beta s + \alpha$ anwenden:

```{r}
log_estim_mass <- beta * d_prot$MD + alpha 
print(log_estim_mass)
```

Nur 3 bis 4 kDa? Kann das sein? Nein, denn wir müssen noch exponentieren -- es handelt sich hier nämlich um logarithmierte Masseschätzungen.

```{r}
estim_mass <- exp( log_estim_mass )
print(estim_mass)
```

Schon eher! Die Schätzungen sind als bei ca. 43 und 45 kDa.

Tragen wir das diagrammatisch auf, in dem wir log-Masse über die Laufstrecke sowie das Regressionsmodell auftragen. Die soeben berechneten Schätzungen, die wenig überraschend auf der Modellgeraden liegen, fügen wir als rote Punkte ein:

```{r}
plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)
points( d_prot$MD, log_estim_mass, col = 2, pch = 16)
```


Schauen wir uns das auf der natürlichen, nicht-logarithmierten Skale an: 

```{r}
plot(d_marker$MD, d_marker$MW, xlab = "Laufstrecke (pxl)", ylab = "Masse (kDa)" )
points( d_prot$MD, estim_mass, col = 2, pch = 16)
```


## mittels `R`-eigenen Funktionen

Verwenden wir jetzt den Funktionsraum von `R`. Wir erhalten die selben, logarithmierten Masseschätzungen mit Hilfe der `predict`-Funktion. Als Argumente benötigt diese das eben berechnete Regressionsmodell und die "neuen" Daten, auf welchen das Modell anzuwenden ist.

```{r}
predict(regrmodel, newdata = d_prot)
```

Diese Funktion kann allerdings noch viel mehr: Sie kann uns Konfidenzintervalle zu diesen Schätzungen angeben. Hier berechnen wir die Intervalle zum statistischen Konfidenzniveau von 95 %:

```{r}
predict(regrmodel, newdata = d_prot,interval = "confidence")
```

Wir erhalten die unteren und obere Schranken der Intervalle. Jedes Intervall liegt dabei **symmetrisch um unsere mittlere Punktschätzung**.

Speichern wir dieses Resultat und tragen es graphisch auf:

```{r}
estim_mass2 <- predict(regrmodel, newdata = d_prot,interval = "confidence")
```

```{r}
plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)
points( d_prot$MD, estim_mass2[,1], col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = estim_mass2[,"lwr"], 
         y1 = estim_mass2[,"upr"],
         col = 2)

```

Wir sehen jetzt wunderbar die Unsicherheit dieser Schätzungen.

Mit der `predict`-Funktion könnten wir noch viel mehr machen. Zum Beispiel können wir ein *Konfidenzband* zu unserem Regressionsmodell bestimmen, in der Art, wie wir es häufig in vielen Publikationen und wissenschaftlichen Abhandlungen finden:

```{r}
md_seq <- seq(50, 700, l = 201)
log_em_seq <- predict(regrmodel, newdata = data.frame(MD = md_seq), interval = "confidence")


plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)

polygon(x = c(md_seq,
              rev(md_seq)),
        y = c(log_em_seq[,"lwr"],
              rev(log_em_seq[,"upr"])),
        col = adjustcolor(4, alpha.f = 0.25),
        border = F)


points( d_prot$MD, estim_mass2[,1], col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = estim_mass2[,"lwr"], 
         y1 = estim_mass2[,"upr"],
         col = 2)


```

Die eigentliche Arbeit in diesem Diagramm verrichtet die `polygon`-Funktion, die ein Polygon entlang einer hypothetischen Menge von Laufstrecken (`md_seq`) und den dazu vorhergesagten unteren und oberen Konfidenzintervallgrenzen zieht. Man sieht somit sehr gut, wie die Konfidenz des Modells bei geringen und großen Laufstrecken *abnimmt* -- je breiter das Band, desto breiter das Intervall plausibler Werte. Unsere Schätzungen liegen allerdings gut in der "Mitte des Modells", nämlich dort, wo konfidentere Aussagen möglich sind. Das ist kein Zufall; das Experiment wurde entsprechend so geplant.

Plotten wir jetzt das Konfidenzintervallband auf der originalen Masse-Skale:

```{r}
em_seq <- exp( log_em_seq )

plot(d_marker$MD, d_marker$MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )

polygon(x = c(md_seq,
              rev(md_seq)),
        y = c(em_seq[,"lwr"],
              rev(em_seq[,"upr"])),
        col = adjustcolor(4, alpha.f = 0.25),
        border = F)


points( d_prot$MD, exp(estim_mass2[,1]), col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = exp(estim_mass2[,"lwr"]), 
         y1 = exp(estim_mass2[,"upr"]),
         col = 2)


```

Auf dieser Skale wird der statistische Nachteil einer linearen Kalibriergeraden auf log-Massen deutlich: Die darauffolgende Exponentiation verzerrt die Konfidenzintervalle und die statistische Power der Bestimmungen sinkt! 😟 

Das sehen wir anhand der folgenden Tabelle: 

```{r}
exp(estim_mass2)
```

Tatsächlich sind die Konfidenzintervalle jeweils 10 kDa breit. Verunsichert sollten wir uns nun fragen: Kann man das nicht besser schätzen? 🤔


## mittels Bayes'scher Statistik

Sind wir mal ehrlich: Wie können die beiden Konfidenzintervalle derartig groß sein? Wir haben immerhin **zwei** Masseschätzungen für **zwei** Laufstreckenmessungen **eines** Proteins erhalten, die zudem jeweils recht **nah beieinander liegen**! Wie kann die Unsicherheit somit derart groß sein?

Hier mein Versuch einer informellen Antwort: Das Problem ist eigentlich recht einfach formuliert -- das statistische Modell *"weiß" nicht*, dass es sich um das selbe Protein handelt. Dadurch verrechnet es nicht die *vereinte* (man sagt auch *gepoolte*) Information, die beide Laufstecken und daraus resultierende Schätzungen liefern. Wie können wir dieses *Pooling* vornehmen und somit bessere Schätzungen erhalten? Das schauen wir uns jetzt an.

Aber Achtung: In diesem Abschnitt wird es fortgeschritten. Wir verwenden nicht nur einen komplexeren statistischen Ansatz sowie ein daraus weniger zugängliches statistisches Modell, sondern auch eine zusätzliche Programmiersprache, um dieses Modell an die Daten fitten zu können. Diese Sprache ist [Stan](https://mc-stan.org/). All das ist nicht prüfungsrelevant!

### das Modell

Statistisch gesehen ist das bis jetzt verwendete lineare Regressionsmodell wie folgt formulierbar:

$$
\log(M) \sim \mathrm{Normal}(\ \mu,\ \sigma),\\
\mu = \beta S + \alpha
$$

Wir nehmen damit an, dass die log-Masse eines Proteins im Erwartungswert, $\mu$, dem linearen Zusammenhang $\beta S + \alpha$ folgt. Der Parameter $\sigma$ gibt um $\mu$ normalverteilte Fehlerstreuung im Sinne einer Standardabweichung an.

Unsere bisher erhaltenen Schätzungen sind nichts anderes als die exponentierten, resultierenden Erwartungswerte für die gemessenen Laufstecken von 368 und 357 Pixel. 

$$
\mu_1 = \beta \cdot 368 + \alpha \\
\mu_2 = \beta \cdot 357 + \alpha
$$

Um diese beiden Erwartungswerte haben wir die ca. $\pm 5$ kDa Konfidenzintervalle erhalten. **Wir ändern den Ansatz nun vollständig**. Als erstes Hinterfragen wir obigen Zusammenhang, denn die statistische Schätzung auf der Log-Skale führt zu Verzerrungen und folglich trägt zur Schätzungsunsicherheit bei. Alternativ nutzen wir jetzt den natürlichen Zusammenhang zwischen mol. Masse und Laufstrecke. Haben wir oben noch die Porportionalität geschrieben als

$$
s \propto \exp(-m)
$$

führt uns dies jetzt einem neuen, natürlicheren Modell mit vier Parametern:

$$
s \sim \mathrm{Normal}(\exp(-\beta m\ +\ \alpha) + \gamma, \sigma),
$$

Wir werden jetzt diese Parameter anhand der Marker-Massen und -Laufstecken schätzen -- wir fitten das Modell. Anschließend wenden wir das gefittete Modell an die Laufstrecken $s_1$ und $s_2$ des zu charakterisierenden Proteins an. Im Kontext Bayes'scher Statistik ist es völlig unproblematisch den natürlichen Zusammenhang zwischen den zwei Laufstrecken $s_1$ und $s_2$ und der unbekannten Masse $\mu_0$ wie folgt abzubilden:

$$
s_1 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma),\\
s_2 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma)
$$

Sehen Sie, wie die Information zwischen den Messpunkten hier zusammenfließt? Wir haben nun einen *Hyperparameter* $\mu_0$ zu schätzen, der dem *Erwartungswert der Erwartungswerte* entspricht ...

![](https://media.tenor.com/w-PCA2wkMQEAAAAM/mind-blown-shocked.gif){fig-align="center" width=300}

Okay, das klingt wild. Aber letztendlich ist es genau das, was wir wollen: Wenn wir die Verteilung der erhaltenen Erwartungswerte (also Masse-Schätzungen) heranziehen, suchen wir jenen Mittelwert der diese Erwartungswerte möglichst plausibel erklären kann. Das heißt, wir suchen ein $\mu_0$ das *statistisch beide Erwartungswerte poolen kann.* 


### die Modelle in Stan implementiert

**Achtung: Ab hier funktionieren die Code-Chunks nur noch auf dem RStudio-Server, da zusätzliche Bibliotheken und Einstellungen benötigt werden. Ab hier ist nichts mehr Pflicht -- ich möchte Ihnen nur zeigen, was technisch so möglich ist.** Sollten Sie den folgenden Abschnitt auf Ihrem Rechner nachbilden wollen, müssen Sie Stan und dessen R-Schnittstelle sowie u. U. weitere Pakete nachinstallieren. Eine Anleitung finden Sie hier: [https://mc-stan.org/cmdstanr/](https://mc-stan.org/cmdstanr/).

Und nun zurück zum Problem und den Stan-Programmen der Modelle. Sie müssen sie nicht programmieren; aber sie in Maschinen-Code [kompilieren](https://de.wikipedia.org/wiki/Compiler) ist nötig:

```{r}
library("cmdstanr")
library("posterior")
```

**Wenn Sie auf dem RStudio-Server arbeiten, müssen Sie jetzt unbedingt die folgende Code-Zeile ausführen:**

```{r eval=FALSE}
set_cmdstan_path("/opt/cmdstan/current/")
```

Wir kompilieren das erste Modell, dass die Marker-Laufstecken an die Massen fittet:

```{r}
m_marker <- cmdstan_model("marker.data.model.stan")
```

Damit haben wir das Modell erhalten, mit dem wir die Nicht-linearen Regressionsparameter des natürlichen Modells anhand der Marker-Daten schätzen können.

Als nächstes kompilieren wir das Modell, mit welchem wir anschließend das Pooling von $\mu_1$ und $\mu_2$ vornehmen.


```{r}
m_pooling <- cmdstan_model("pooling.model.stan")
```

Und hier sind die Source Codes:

```{r}
print( m_marker )
```

```{r}
print( m_pooling )
```

Gleich fitten wir das erste Modell. Allerdings muss hier gesagt werden, dass das Fitting deutlich stabiler läuft, wenn wir die Daten etwas skalieren. Aus didaktischen Gründen nehmen wir eine einfache Skalierung vor, in dem wir die Messwerte durch angenehme Referenzwerte, hier 700 Pixel für Laufweiten und 100 kDa für mol. Massen, teilen. Das tun wir jetzt und speichern die neuen, skalierten Daten in einer Listenstruktur:

```{r}
md <- list(
  MD = d_marker$MD / 700,
  MW = d_marker$MW / 100
  
)
```

Das Skalieren ändert nichts am natürlichen Zusammenhang zwischen den Daten:

```{r}
plot(md$MW, md$MD)
```

Fitten wir jetzt das Modell. In den nächsten Sekunden wird Ihr Computer versuchen, Stichprobenwerte der Modellparameter zu finden. Diese Stichprobenwerte werden dabei derartig gewählt, so dass sie die Verteilungen der Modellparameter empirisch annähern. Wir können sie interpretieren als *plausible* Parameterwerte, die die beobachten Marker-Laufstrecken möglichst gut erklären können, gegeben die molekularen Marker-Massen und das obige Modell des natürlichen Zusammenhangs zwischen diesen Größen.

Im folgenden Code-Chunk starten wir dieses *Sampling*, wobei wir eine Stichpobe von 5.000 Werten erzeugen lassen:

```{r samplem1}
f <- m_marker$sample( data = md, 
               chains = 1, # one sampling chain ...
               seed = 1, # starting at fixed value for reproducibility, ...
               adapt_delta = 0.999, # generates cautiously ...
               iter_warmup = 5000, # 5000 samples for warming up 
               iter_sampling = 5000, # and the actual 5000 samples after warmup, we' ll use later.
               refresh = 1000 # The processing status is printed after every 1000 samples
               )
```

Nach wenigen Sekunden haben wir 5,000 Werte erhalten. Hier können Sie in die Parameterwerte hineinschauen ...

```{r}
f
```

... aber so richtig sehen wir darin nichts. Es sind nur Zahlen. Stattdessen visualisieren wir die gefitteten Werte:

```{r}
draws <- as_draws_df(f$draws())

mw_seq <- seq(0, 150, l = 151) / 100
plot(md$MW, md$MD)

# add 100 regressions lines drawn from the 5,000 samples
for(i in 1:100) lines( mw_seq, exp(-draws$beta[i] * mw_seq + draws$alpha[i]) + draws$gamma[i], col = adjustcolor(4, 0.2) )


points(md$MW, md$MD, lwd = 2)

```


Bevor wir das zweite, poolende Modell fitten und unsere finale Masseschätzung $\mu_0$ erhalten, betrachten wir die Güte des gerade gefitteten Modells. Nehmen wir hierfür die skalierten Laufstrecken des unbekannten Proteins und "stecken" sie in das gefittete Modell. Die 700 ist der selbe, schon oben verwendete Referenzwert. Die benötigten Parameterwerte übergeben wir der folgenden Listenstruktur gleich mit. Packen wir alles in eine Listenstruktur:

```{r}
md2 <- list(
  # scaled distances of the protein
  MD1 = d_prot$MD[1] / 700,
  MD2 = d_prot$MD[2] / 700,
  
  # samples we just obtained:
  N     = length( draws$beta ),
  beta  = draws$beta,
  alpha = draws$alpha,
  gamma = draws$gamma,
  sigma = draws$sigma
)
```

Rechnen wir die entsprechenden Masseschätzungen $\mu_1$ und $\mu_2$ aus, in dem wir den natürlichen Zusammenhang nach der Masse umstellen. Die Multiplikation mit 100 ist nötig, um die Schätzung auf kDa zurück zu skalieren.

```{r}
mu1 <- 100 * (log(md2$MD1 - draws$gamma) - draws$alpha) / -draws$beta  
mu2 <- 100 * (log(md2$MD2 - draws$gamma) - draws$alpha) / -draws$beta
```

Die folgenden Histogramme zeigen, dass unser natürlicher Ansatz schon deutlich sicherere Schätzungen liefert. Der Großteil der Werte liegen in Intervallen mit Breiten von nur noch 3 bis 4 kDa.  

```{r}
hist(mu1, breaks = 100)
hist(mu2, breaks = 100)
```

In diesen [Quantilen](https://de.wikipedia.org/wiki/Quantil_(Wahrscheinlichkeitstheorie)) liegen 95 % der Werte:

```{r}
PI1 <- quantile( mu1, probs = c(0.025, 0.975) )
PI2 <- quantile( mu2, probs = c(0.025, 0.975) )

PI1
PI2
```

Grafisch sieht das wie folgt aus:

```{r}
plot(md$MW, md$MD)
for(i in 1:100) lines( mw_seq, exp(-draws$beta[i] * mw_seq + draws$alpha[i]) + draws$gamma[i], col = adjustcolor(4, 0.2) )

segments(y0 = md2$MD1, x0 = PI1[1] / 100, x1 = PI1[2] / 100, col = 2, lwd = 3)
segments(y0 = md2$MD2, x0 = PI2[1] / 100, x1 = PI2[2] / 100, col = 2, lwd = 3)

points(md$MW, md$MD, lwd = 2)
```

Und jetzt folgt das zweite Modell, mit welchem wir "über die roten Segmente im Diagramm" poolen.


### Anwendung des Pooling-Modells

Kurzes Recap bis hierhin: Wir haben bis jetzt Stichprobenwerte der Parameter $\beta$, $\gamma$ $\alpha$ (sowie den Fehlerparameter $\sigma$) des "natürlichen Zusammenhang-Modells" 

$$
s \sim \mathrm{Normal}(\exp(-\beta m\ +\ \alpha) + \gamma, \sigma),
$$

erhalten.

Diese Stichprobenwerte stecken wir jetzt in das zweite Modell, welches über die gemessenen Laufstrecken $s_1$ und $s_2$ poolt und den für uns spannenden Parameter $\mu_0$ inferiert. Auch hier werden wir wieder "nur" Stichprobenwerte für $\mu_0$ erhalten.

$$
s_1 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma),\\
s_2 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma)
$$

Wir fitten das Modell. Dabei erzeugen wir 3.000 Stichprobenwerte für $\mu_0$

```{r}
f2 <- m_pooling$sample(data = md2, chains = 1, refresh = 1000, iter_warmup = 2000, iter_sampling = 3000, adapt_delta = 0.999)
```

Und hier sind die Stichprobenwerte von $\mu_0$. Achtung: Sie müssen den Wert mit dem Referenzwert von 100 gedanklich multiplizieren.

```{r}
f2
```

```{r}
draws2 <- as_draws_df( f2$draws())
```

Hier ist ein Histogramm des gepoolten $\mu_0$

```{r}
hist(100 * draws2$mu0, breaks = 100)
```

Wir erhalten demnach 41 bis 42 kDa als Schätzung. Das Pooling und der Ansatz, den natürlichen Zusammenhang zwischen Daten statistisch abzubilden, hat uns zu einer deutlich sichereren Schätzung geführt. Fun fact: Eine massensprektrometrische Messung ergab eine Masse von 42 kDa.


::: {.callout-note appearance="simple"}
## Literaturempfehlung

Für alle die sich mit der Materie näher auseinandersetzen möchten, empfehle ich dieses Buch, das mit vielen interessanten und eingängigen `R`-Beispielen aufwarten kann: [A. Johnson *et al*, Bayes rules! (2021). CRC Press](https://www.bayesrulesbook.com/)

Ebenfalls spannend unter tiefergehend ist [R. McElreath, Statistical rethinking (2020). Taylor & Francis](https://www.taylorfrancis.com/books/mono/10.1201/9780429029608/statistical-rethinking-richard-mcelreath). Für den Blick unter die (mathematische) Motorhaube, empfehle ich [A. Gelman *et al*, Bayesian Data Analysis (2013, online edition 2025). Chapman & Hall](https://sites.stat.columbia.edu/gelman/book/). 

:::


